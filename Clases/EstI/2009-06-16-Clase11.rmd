---
layout: post
title: "Clase 11"
main-class: 'clase'
permalink: /EstadisticaI/EstI:title.html
tags:

introduction: |
  Distribuciones de probabilidad discreta: </br>
  - Distribución Binomial </br>
  - Distribución Hipergeométrica </br>
  - Distribución Geométrica </br>
  - Distribución Binomial Negativa
  
  
header-includes:
   - \usepackage{amsmath,amssymb,amsthm,amsfonts}
   - \usepackage[sectionbib]{natbib}
   - \usepackage[hidelinks]{hyperref}
output:
  md_document:
    variant: markdown_strict+backtick_code_blocks+autolink_bare_uris+ascii_identifiers+tex_math_single_backslash
    preserve_yaml: TRUE
always_allow_html: yes   
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding,
  output_dir = "../../EstadisticaI/_posts/", output_format = "all"  ) })
bibliography: "../../referencias.bib"
csl: "../../apa.csl"
---

```{r knitr_init, echo=FALSE, cache=FALSE}
library(knitr)
## Global options
opts_chunk$set(echo=TRUE,
               cache=TRUE,
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE,
               fig.path = paste0("../../EstadisticaI/images/", "Clase11"),
               cache.path = "../../EstadisticaI/cache/",
               cache = FALSE)

```

## Distribuciones de probabilidad discretas
### Proceso Bernoulli
Un proceso Bernoulli es aquel que cumple

1. El experimento consta de ensayos repetidos.
2. Cada ensayo produce un resultado que se puede clasificar como éxito o fracaso.
3. La probabilidad de éxito se denota por `$p$`, mientras que la probabilidad de fracaso se denota por `$q=1-p$`, y estas probabilidades permanecen constante de un ensayo a otro.
4. Los ensayos repetidos son independientes.

### Ensayo de Bernoulli
Si la probabilidad de éxito de un experimento es `$p$`, entonces se tendrá que la probabilidad de fallo será `$1-p$`, y por tanto, la función de probabilidad de la variable aleatoria `$X\sim Be(p)$` de un ensayo Bernoulli es
`\begin{align*}
f(x) = p^x(1-p)^{1-x} \quad \quad x=0,1
\end{align*}`

#### Teorema
Si `$X\sim Be(p)$`, entonces se puede probar que la media y la varianza de la variable aleatoria `$X$` están dadas por 
`\begin{align*}
\mathbb{E}(X)=p \quad \quad Var(X)=p(1-p)
\end{align*}`

### Distribución Binomial
Si `$X$` es la variable aleatoria del número de éxitos de `$n$` ensayos de Bernoulli, con probabilidad de éxito `$p$`, entonces se dice que `$X\sim b(n,p)$` tal que
`\begin{align*}
f(x) = \left(\begin{array}{c}n\\ x\end{array}\right)p^x(1-p)^{n-x} \quad \quad x=0,1,\ldots,n
\end{align*}`

**Nota:** Esta distribución es usada cuando se realiza muestreo **con reemplazo** o en poblaciones infinitas en donde es posible **suponer que la probabilidad de éxito `$p$` es la misma** en cada ensayo Bernoulli.

#### Teorema
Si `$X\sim b(n,p)$`, entonces se puede probar que la media y varianza de la variable aleatoria `$X$` están dadas por
`\begin{align*}
\mathbb{E}(X)=np \quad \quad Var(X)=np(1-p)
\end{align*}`

### Distribución Hipergeométrica
Si `$X$` es el número de éxitos de una muestra completamente aleatoria de tamaño `$n$` extraída de una población `$N$` compuesta por `$M$` éxitos y `$(N-M)$` fracasos, entonces la distribución de probabilidad de `$X$`, denotada por `$h(N,M,n)$`, está dado por 
`\begin{align*}
h(N,M,n)=\frac{\left(\begin{array}{c}M\\ x\end{array}\right) \left(\begin{array}{c}N-M\\ n-x\end{array}\right)}{\left(\begin{array}{c}N\\ n\end{array}\right)}
\end{align*}`

con `$x$` un entero que satisface la condición max`$\{0, M-(N-n)\leq x \leq$`min`${M,n}\}$`.

**Nota:** Esta distribución es usada cuando se realiza muestreo **sin reemplazo**, en poblaciones finitas donde hay `$M$` éxitos de un total de `$N$` objetos, de los cuales se seleccionan `$n$` objetos a la vez.

#### Teorema
Si `$X\sim h(N,M,n)$`, entonces se puede probar que la media y varianza de la variable aleatoria `$X$` están dadas por
`\begin{align*}
\mathbb{E}(X)=n\frac{M}{N} \quad \quad Var(X)=\left(\frac{N-n}{N-1}\right)\left(\frac{M}{N}\right)\left(1-\frac{M}{N}\right)
\end{align*}`

### Experimento Binomial Negativo
Consideremos un experimento con las mismas propiedades de un experimento binomial, solo que en este caso, las pruebas se repetirán hasta que ocurra un número fijo de éxitos. Por lo tanto en vez de encontrar la probabilidad de `$X$` éxitos en `$n$` pruebas, donde `$n$` es fija, ahora nos interesa la probabilidad de que ocurra el `$k$`-ésimo éxito en la `$X$`-ésima prueba.

### Distribución Geométrica
Sea  `$X$` el número de ensayos necesarios para generar un éxito (`$k=1$`), entonces se dice que `$X\sim g(p)$` si su función de probabilidad es de la forma
`\begin{align*}
g(p) = p(1-p)^{x-1} \quad \quad x = 1,2,\ldots
\end{align*}`

#### Teorema
Si `$X\sim g(p)$`, entonces se puede probar que la media y varianza de la variable aleatoria `$X$` están dadas por
`\begin{align*}
\mathbb{E}(X)=\frac{1}{p} \quad \quad Var(X)=\frac{1-p}{p^2}
\end{align*}`

### Distribución Binomial Negativa
Sea `$X$` el número de ensayos necesarios para generar `$k$` éxitos en un experimento Binomial Negativo, entonces se dice que `$X\sim b^*(k,p)$` si su función de probabilidad es de la forma
`\begin{align*}
b^*(p) = \left(\begin{array}{c}x-1\\ k-1\end{array}\right)p^k(1-p)^{x-k} \quad \quad x=k, k+1, \ldots
\end{align*}`

#### Teorema
Si `$X\sim b^*(k,p)$`, entonces se puede probar que la media y varianza de la variable aleatoria `$X$` están dadas por
`\begin{align*}
\mathbb{E}(X)=\frac{k}{p} \quad \quad Var(X)=k\frac{(1-p)}{p^2}
\end{align*}`

### Proceso Poisson
Un proceso Poisson es aquel que cumple

1. El número de resultados que ocurren en un intervalo o región específica es independiente del número que ocurre en cualquier otro intervalo de tiempo o región del espacio disjunto.
2. La probabilidad de que ocurra un solo resultado durante un intervalo de tiempo corto o región pequeña es proporcional a la longitud del intervalo o al tamaño de la región, y no depende del número de resultados que ocurren por fuera de este intervalo de tiempo o región.
3. La probabilidad de que ocurra más de un resultado en tal intervalo o región pequeña es insignificante.

### Distribución Poisson
El número de sucesos que ocurren en un intervalo de tiempo o región específica, es una variable aleatoria `$X$` con distribución de probabilidad Poisson, tal que
`\begin{align*}
f(x) = \frac{e^{-\lambda}\lambda^x}{x!} \quad \quad x=0,1,\ldots
\end{align*}`
donde `$\lambda$` es el parámetro de la distribución y representa el número promedio de sucesos por unidad de tiempo o región específica.

#### Teorema
Si `$X\sim P(\lambda)$` entonces se puede probar que la media y varianza de la variable aleatoria `$X$` están dadas por
`\begin{align*}
\mathbb{E}(X)=\lambda \quad \quad Var(X)=\lambda
\end{align*}`