---
layout: post
title: "Clase 18"
main-class: 'clase'
permalink: /EstadisticaII/EstII:title.html
tags:

introduction: |
              Análisis de Regresión <br>
              - Prueba de hipótesis <br>
              - Intervalos de confianza <br>

header-includes:
   - \usepackage{amsmath,amssymb,amsthm,amsfonts}
   - \usepackage[sectionbib]{natbib}
   - \usepackage[hidelinks]{hyperref}
output:
  md_document:
    variant: markdown_strict+backtick_code_blocks+autolink_bare_uris+ascii_identifiers+tex_math_single_backslash
    preserve_yaml: TRUE
always_allow_html: yes   
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding,
  output_dir = "../../EstadisticaII/_posts/", output_format = "all"  ) })
bibliography: "../../referencias.bib"
csl: "../../apa.csl"
link-citations: yes
---

```{r knitr_init, echo=FALSE, cache=FALSE}
library(knitr)
## Global options
opts_chunk$set(echo=TRUE,
               cache=TRUE,
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE,
               fig.path = paste0("../../EstadisticaII/images/", "Clase18"),
               cache.path = "../../EstadisticaII/cache/",
               cache = FALSE)

```

## Análisis de Regresión
### Prueba de hipótesis
Note que `$\hat{\beta}_0$` y `$\hat{\beta}_1$` son combinaciones lineales de `$y_i$`, que son normales e independientes, entonces por propiedades de la distribución normal se obtiene que
`\begin{align*}
 \hat{\beta}_0 &\sim N\left(\beta_0, \left[\frac{1}{n} + \frac{\bar{X}^2}{S_{xx}}\right]\sigma^2_e\right)  \\
 \hat{\beta}_1 &\sim N\left(\beta_1, \frac{\sigma^2_e}{S_{xx}}\right)
\end{align*}`

Entonces, para realizar el contraste de hipótesis `$\beta_0$` y `$\beta_1$` de la forma
`\begin{align*}
 H_0: \beta_0 = \beta_{00} \quad & \quad H_0: \beta_1 = \beta_{10}\\
 H_1: \beta_0 \neq \beta_{00} \quad & \quad H_1: \beta_1 \neq \beta_{10}\\
\end{align*}`

se tendrá que los estadísticos de prueba serían de la forma
`\begin{align*}
 t_{\hat{\beta}_{0}}= \frac{\hat{\beta}_0 - \beta_{00}}{\sqrt{\left[\frac{1}{n} + \frac{\bar{X}^2}{S_{xx}}\right]\sigma^2_e}} \quad & \quad t_{\hat{\beta}_{1}}= \frac{\hat{\beta}_1 - \beta_{10}}{\sqrt{\frac{\sigma^2_e}{S_{xx}}}}
\end{align*}`

y para tomar la decisión si se rechaza o no la hipótesis se emplean la región crítica la cual está dada por
`\begin{align*}
 RC:\{t|t < -t_{\frac{\alpha}{2}, n-2} \quad ó \quad t > t_{\frac{\alpha}{2}, n-2}\}
\end{align*}`
en donde si `$t_{\hat{\beta}_{i}} \in RC$` para `$i=0,1$`, se rechaza `$H_0$`. Mientras que el P-valor está dada por
`\begin{align*}
 \text{P-valor}= 2\mathbb{P}(t_{n-2}\geq |t_{\hat{\beta}_i}|) \quad \text{ para } i = 0,1
\end{align*}`

en donde si P-valor es menor al nivel de significancia `$\alpha$`, entonces se rechaza `$H_0$`.

**Nota**

* Usualmente se quiere probar si `$\beta_{0}=0$` y `$\beta_{1}=0$`, ya que se quiere probar a significancia de los parámetros.
* Observe que rechazar `$H_0:\beta_{1}=0$`, en la prueba de significancia, permite afirmar que la relación entre las variables `$Y$` y `$X$` se pueden aproximar mediante una linea recta. 
* Si no se rechaza `$H_0:\beta_{1}=0$`, significa que independiente del valor de `$X$`, el valor de `$Y$` será el mismo, y por tanto, no habrá una relación lineal entre `$Y$` y `$X$`.

### Intervalo de confianza
El intervalo de confianza par `$\beta_0$` está dado por
`\begin{align*}
 \hat{\beta}_0 - t_{\frac{\alpha}{2}, n-2} \sqrt{\left(\frac{1}{n}+\frac{\bar{X}^2}{S_{xx}}\right)\sigma^2_{e}} < \beta_0 < \hat{\beta}_0 + t_{\frac{\alpha}{2}, n-2} \sqrt{\left(\frac{1}{n}+\frac{\bar{X}^2}{S_{xx}}\right)\sigma^2_{e}}
\end{align*}`

para el caso de `$\beta_1$`, el intervalo de confianza está dado por
`\begin{align*}
 \hat{\beta}_1 - t_{\frac{\alpha}{2}, n-2} \sqrt{\frac{\sigma^2_{e}}{S_{xx}}} < \beta_1 < \hat{\beta}_1 + t_{\frac{\alpha}{2}, n-2} \sqrt{\frac{\sigma^2_{e}}{S_{xx}}}
\end{align*}`

**Nota**

Si el Intervalo de Confianza de `$\beta_1$` no contiene el cero, se puede afirmar que la variable `$Y$` está relacionada linealmente con la variable independiente `$X$`. En caso contrario, no existe relación lineal entre `$X$` y `$Y$`, es decir, la variable `$X$` no es adecuada para predecir el comportamiento de `$Y$`.
