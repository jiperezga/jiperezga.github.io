---
layout: post
title: "Clase 18"
main-class: 'clase'
permalink: /EstadisticaII/EstII:title.html
tags:

introduction: |
              Análisis de Regresión <br>
              - Prueba de hipótesis <br>
              - Intervalos de confianza <br>
              Supuestos Regresión Lineal Simple <br>
              - Linealidad <br>
              - Normalidad <br>
              - Homocedasticidad <br>
              - Independencia 

header-includes:
   - \usepackage{amsmath,amssymb,amsthm,amsfonts}
   - \usepackage[sectionbib]{natbib}
   - \usepackage[hidelinks]{hyperref}
output:
  md_document:
    variant: markdown_strict+backtick_code_blocks+autolink_bare_uris+ascii_identifiers+tex_math_single_backslash
    preserve_yaml: TRUE
always_allow_html: yes   
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding,
  output_dir = "../../EstadisticaII/_posts/", output_format = "all"  ) })
bibliography: "../../referencias.bib"
csl: "../../apa.csl"
link-citations: yes
---

```{r knitr_init, echo=FALSE, cache=FALSE}
library(knitr)
## Global options
opts_chunk$set(echo=TRUE,
               cache=TRUE,
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE,
               fig.path = paste0("../../EstadisticaII/images/", "Clase18"),
               cache.path = "../../EstadisticaII/cache/",
               cache = FALSE)

```

## Análisis de Regresión
### Prueba de hipótesis
Note que `$\hat{\beta}_0$` y `$\hat{\beta}_1$` son combinaciones lineales de `$y_i$`, que son normales e independientes, entonces por propiedades de la distribución normal se obtiene que
`\begin{align*}
 \hat{\beta}_0 &\sim N\left(\beta_0, \left[\frac{1}{n} + \frac{\bar{X}^2}{S_{xx}}\right]\sigma^2_e\right)  \\
 \hat{\beta}_1 &\sim N\left(\beta_1, \frac{\sigma^2_e}{S_{xx}}\right)
\end{align*}`

Entonces, para realizar el contraste de hipótesis `$\beta_0$` y `$\beta_1$` de la forma
`\begin{align*}
 H_0: \beta_0 = \beta_{00} \quad & \quad H_0: \beta_1 = \beta_{10}\\
 H_1: \beta_0 \neq \beta_{00} \quad & \quad H_1: \beta_1 \neq \beta_{10}\\
\end{align*}`

se tendrá que los estadísticos de prueba serían de la forma
`\begin{align*}
 t_{\hat{\beta}_{0}}= \frac{\hat{\beta}_0 - \beta_{00}}{\sqrt{\left[\frac{1}{n} + \frac{\bar{X}^2}{S_{xx}}\right]\sigma^2_e}} \quad & \quad t_{\hat{\beta}_{1}}= \frac{\hat{\beta}_1 - \beta_{10}}{\sqrt{\frac{\sigma^2_e}{S_{xx}}}}
\end{align*}`

y para tomar la decisión si se rechaza o no la hipótesis se emplean la región crítica la cual está dada por
`\begin{align*}
 RC:\{t|t < -t_{\frac{\alpha}{2}, n-2} \quad ó \quad t > t_{\frac{\alpha}{2}, n-2}\}
\end{align*}`
en donde si `$t_{\hat{\beta}_{i}} \in RC$` para `$i=0,1$`, se rechaza `$H_0$`. Mientras que el P-valor está dada por
`\begin{align*}
 \text{P-valor}= 2\mathbb{P}(t_{n-2}\geq |t_{\hat{\beta}_i}|) \quad \text{ para } i = 0,1
\end{align*}`

en donde si P-valor es menor al nivel de significancia `$\alpha$`, entonces se rechaza `$H_0$`.

**Nota**

* Usualmente se quiere probar si `$\beta_{0}=0$` y `$\beta_{1}=0$`, ya que se quiere probar a significancia de los parámetros.
* Observe que rechazar `$H_0:\beta_{1}=0$`, en la prueba de significancia, permite afirmar que la relación entre las variables `$Y$` y `$X$` se pueden aproximar mediante una linea recta. 
* Si no se rechaza `$H_0:\beta_{1}=0$`, significa que independiente del valor de `$X$`, el valor de `$Y$` será el mismo, y por tanto, no habrá una relación lineal entre `$Y$` y `$X$`.

### Intervalo de confianza
El intervalo de confianza par `$\beta_0$` está dado por
`\begin{align*}
 \hat{\beta}_0 - t_{\frac{\alpha}{2}, n-2} \sqrt{\left(\frac{1}{n}+\frac{\bar{X}^2}{S_{xx}}\right)\sigma^2_{e}} < \beta_0 < \hat{\beta}_0 + t_{\frac{\alpha}{2}, n-2} \sqrt{\left(\frac{1}{n}+\frac{\bar{X}^2}{S_{xx}}\right)\sigma^2_{e}}
\end{align*}`

para el caso de `$\beta_1$`, el intervalo de confianza está dado por
`\begin{align*}
 \hat{\beta}_1 - t_{\frac{\alpha}{2}, n-2} \sqrt{\frac{\sigma^2_{e}}{S_{xx}}} < \beta_1 < \hat{\beta}_1 + t_{\frac{\alpha}{2}, n-2} \sqrt{\frac{\sigma^2_{e}}{S_{xx}}}
\end{align*}`

**Nota**

Si el Intervalo de Confianza de `$\beta_1$` no contiene el cero, se puede afirmar que la variable `$Y$` está relacionada linealmente con la variable independiente `$X$`. En caso contrario, no existe relación lineal entre `$X$` y `$Y$`, es decir, la variable `$X$` no es adecuada para predecir el comportamiento de `$Y$`.

## Revisión de supuestos del modelo
Existe un serie de supuestos que debían se corroborados para asegurar que el modelo de regresión lineal esté correctamente desarrollado. Entre los cuales tenemos:

### Linealidad
Uno de los primeros supuestos que se debe probar es que haya una relación lineal entre la variable dependiente y las independientes, y para ello se debe examinar la relación entre los términos de error (residuales) y los valores ajustados, en donde:
1. Se verifica que la media de los residuales tenga media 0, y se distribuya de forma aleatoria a lo largo de la línea horizontal.
2. No debe exhibirse en el diagrama de dispersión una tendencia clara o forma sistemática.
3. La dispersión de los residuos debe ser aproximadamente constante a lo largo de los valores ajustados

Para probar en <tt>R</tt> el supuesto de linealidad, es posible calcularle la relación entre los residuales y los valores ajustados mediante la función <tt>plot(modelo,1)</tt>.

Una forma alternativa sería generando el gráfico entre la variable dependiente e independiente con la función <tt>plot(x,y)</tt>, o usando la prueba RESET de Ramsey que permite validar si un modelo lineal tiene la forma funcional correcta o si se han omitido términos no lineales o interacciones entre las variables. Esta prueba puede realizarse en <tt>R</tt> mediante la función <tt>resettest(modelo)</tt> de la librería <tt>lmtest</tt>. 

### Normalidad
El segundo supuesto que se debe considerar es que los términos de error (residuales) de la regresión se distribuyan normalmente, debido a que si éstos se distribuyen normalmente, se tendrá que la variable respuesta $Y$ también se distribuya normalmente, habilitando que las ecuaciones del cálculo de los estimadores sean correctas. 

Para probar la normalidad del los residuales es cuestión de usar alguna de las pruebas de normalidad vistas en pruebas de bondad de ajuste, tales como la Shapiro-Wilk, Anderson Darling, Cramer-Von Mises <a href="https://jiperezga.github.io/EstadisticaII/EstIIClase16.html#prueba-de-bondad-de-ajuste" target="\_blank">(Ver Clase 16)</a>, o de forma gráfica mediante la construcción de un QQ-Plot que puede generarse con la función <tt>plot(modelo, 2)</tt>.

### Homocedasticidad
El tercer supuesto busca probar que la variabilidad de los residuales estudentizados sea constante a lo largo de las observaciones, es decir, que no vaya aumentando (disminuyendo) a medida que aumentan (disminuyen) los valores ajustados.

Para probar este supuesto, es posible plantearse un gráfico de diagrama de dispersión de los residuos estudentizados frente a los valores ajustados usando la función <tt>plot(modelo, 3)</tt>.

Otra alternativa es mediante el uso de la prueba Breusch-Pagan, la cual busca probarla hipótesis de que los términos de error son homocedásticos, contra la alternativa, de que los términos de error son heterocedásticos. Esta prueba puede realizarse en <tt>R</tt> con la función <tt>bptest(modelo)</tt> de la librería <tt>lmtest</tt>.

### Independencia 
Finalmente, el cuarto supuesto del modelo de regresión lineal lo que busca es verificar si los residuales están incorrelacionadas, y por tanto, se tendrá que la covarianza de los términos de error será igual a $0$.

Para para probar este supuesto, es posible plantear una gráfica entre los residuos contra el orden de las observaciones, para verificar que si los residuos están distribuidos de manera aleatoria sin mostrar ningún patrón o tendencia sistemática a lo largo del orden de las observaciones, lo cual indicaría que el supuesto de independencia se cumple. Lo anterior puede realizarse en <tt>R</tt> mediante la función <tt>plot(residuales(modelo), type = "o")</tt>.

Alternativamente, se suele utilizar la prueba Durbin-Watson, la cual verifica la hipótesis de que hay independencia para los términos deerror contra la alternativa de que hay dependencia. Para realizar esta prueba en <tt>R</tt> se puede utilizar la prueba Durbin-Watson con la función <tt>dwtest(modelo)</tt> de la librería <tt>lmtest</tt>.
