---
layout: post
title: "Clase 02"
main-class: 'clase'
permalink: /ProbabilidadeInferencia/PeIE:title.html
tags:

introduction: |
  Probabilidad condicional. <br/>
  Regla multiplicativa. <br/>
  Independencia entre eventos. <br/>
  Probabilidad total. <br/>
  Teorema de Bayes.
header-includes:
   - \usepackage{amsmath,amssymb,amsthm,amsfonts}
   - \usepackage[sectionbib]{natbib}
   - \usepackage[hidelinks]{hyperref}
output:
  md_document:
    variant: markdown_strict+backtick_code_blocks+autolink_bare_uris+ascii_identifiers+tex_math_single_backslash
    preserve_yaml: TRUE
always_allow_html: yes   
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding,
  output_dir = "../../ProbabilidadeInferencia/_posts/", output_format = "all"  ) })
bibliography: "../../referencias.bib"
csl: "../../apa.csl"
---

```{r knitr_init, echo=FALSE, cache=FALSE}
library(knitr)
## Global options
opts_chunk$set(echo=TRUE,
               cache=TRUE,
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE,
               fig.path = paste0("../../ProbabilidadeInferencia/images/", "Clase02"),
               cache.path = "../../ProbabilidadeInferencia/cache/",
               cache = FALSE)

```

## Probabilidad Condicional
Es la probabilidad de que ocurra un evento `$A$`, cuando se sabe que ya ocurrió un evento `$B$`, se denota por `$\mathbb P(A|B)$` y se define como
`\begin{align*}
\mathbb P(A|B)=\frac{\mathbb P(A\cap B)}{\mathbb P(B)}; \text{ con } \mathbb P(B)>0
\end{align*}`

## Regla multiplicativa
Si un experimento estadístico pueden ocurrir dos eventos `$A$` y `$B$`, entonces
`\begin{align*}
\mathbb P(A\cap B)=\mathbb P(B)\mathbb P(A|B)=\mathbb P(A)\mathbb P(B|A); \text{ para } \mathbb P(B)>0 \text{ y } \mathbb P(A)>0
\end{align*}`

En general, si pueden ocurrir los eventos `$A_1, A_2, \ldots, A_n$`, entonces
`\begin{align*}
\mathbb P\left(\underset{i=1}{\stackrel{n}{\cap}} A_i\right)=\mathbb P(A_1) \mathbb P(A_2|A_1)\mathbb P(A_3|A_1\cap A_2)\ldots \mathbb P\left(A_n \Big|\underset{i=1}{\stackrel{n-1}{\cap}} A_i\right)
\end{align*}`

## Independencia entre eventos
Si un experimento estadístico pueden ocurrir dos eventos `$A$` y `$B$` que son independientes entre si, entonces
`\begin{align*}
\mathbb P(A|B)=\mathbb P(A) \text{ o } \mathbb P(A|B)=\mathbb P(B)
\end{align*}`

lo cual es equivalente a
`\begin{align*}
\mathbb P(A\cap B)=\mathbb P(A)\mathbb P(B)
\end{align*}`

En cualquier otro caso, los eventos `$A$` y `$B$` son dependientes.

## Probabilidad total
Sean `$A_1, A_2, \ldots, A_k$` eventos mutuamente excluyentes `$(A_i\cap A_j = \oslash, para i\neq j)$` y exhaustivos `$\left(\underset{i=1}{\stackrel{k}{\cup}} A_i = S\right)$`, entonces para cualquier otro evento `$B$`
`\begin{align*}
\mathbb P(B) &= \mathbb P(B\cap A_1)+\mathbb P(B\cap A_2)+\ldots+\mathbb P(B\cap A_k) \\
\mathbb P(B) &= \mathbb P(A_1)\mathbb P(B|A_1)+\mathbb P(A_2)\mathbb P(B|A_2)+\ldots+\mathbb P(A_k)\mathbb P(B|A_k)
\end{align*}`

## Teorema de Bayes
Sea `$A_1, A_2, \ldots, A_k$` eventos mutuamente excluyentes y exhaustivos, con probabilidades previas `$\mathbb P(A_i)$`, para `$i=1,2,\ldots,k$`, entonces para cualquier evento `$B$` para el cual `$\mathbb P(B)>0$`, la probabilidad posterior `$A_j$` dado que `$B$` ha ocurrido está dada por
`\begin{align*}
\mathbb P(A_j|B) &= \frac{\mathbb P(A_j\cap B)}{\mathbb P(B)} \\
\mathbb P(A_j|B) &= \frac{\mathbb P(A_j)\mathbb P(B|A_j)}{\sum_{i=1}^k \mathbb P(A_i)\mathbb P(B|A_i)}
\end{align*}`
