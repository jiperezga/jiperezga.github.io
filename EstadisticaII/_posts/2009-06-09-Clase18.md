---
layout: post
title: "Clase 18"
main-class: 'clase'
permalink: /EstadisticaII/EstII:title.html
tags:

introduction: |
              Análisis de Regresión <br>
              - Prueba de hipótesis <br>
              - Intervalos de confianza <br>
              - Análisis de varianza <br>

              
header-includes:
   - \usepackage{amsmath,amssymb,amsthm,amsfonts}
   - \usepackage[sectionbib]{natbib}
   - \usepackage[hidelinks]{hyperref}
output:
  md_document:
    variant: markdown_strict+backtick_code_blocks+autolink_bare_uris+ascii_identifiers+tex_math_single_backslash
    preserve_yaml: TRUE
always_allow_html: yes   
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding,
  output_dir = "../../EstadisticaII/_posts/", output_format = "all"  ) })
bibliography: "../../referencias.bib"
csl: "../../apa.csl"
link-citations: yes
---







## Análisis de Regresión

### Prueba de hipótesis

Note que `$\hat{\beta}_0$` y `$\hat{\beta}_1$` son combinaciones
lineales de `$y_i$`, que son normales e independientes, entonces por
propiedades de la distribución normal se obtiene que
`\begin{align*}  \beta_0 &\sim N\left(\beta_0, \left[\frac{1}{n} + \frac{\bar{X}^2}{S_{xx}}\right]\sigma^2\right)  \\  \beta_1 &\sim N\left(\beta_1, \frac{\sigma^2}{S_{xx}}\right) \end{align*}`

Entonces, para realizar el contraste de hipótesis `$\beta_0$` y
`$\beta_1$` de la forma
`\begin{align*}  H_0: \beta_0 = \beta_{00} \quad & \quad H_0: \beta_1 = \beta_{10}\\  H_1: \beta_0 \neq \beta_{00} \quad & \quad H_1: \beta_1 \neq \beta_{10}\\ \end{align*}`

se tendrá que los estadísticos de prueba serían de la forma
`\begin{align*}  t_{\hat{\beta}_{0}}= \frac{\hat{\beta}_0 - \beta_{00}}{\sqrt{\left[\frac{1}{n} + \frac{\bar{X}^2}{S_{xx}}\right]\sigma^2_e}} \quad & \quad t_{\hat{\beta}_{1}}= \frac{\hat{\beta}_1 - \beta_{10}}{\sqrt{\frac{\sigma^2_e}{S_{xx}}}} \end{align*}`

y para tomar la decisión si se rechaza o no la hipótesis se emplean la
región crítica la cual está dada por
`\begin{align*}  RC:\{t|t < -t_{\frac{\alpha}{2}, n-2} \quad ó \quad t > t_{\frac{\alpha}{2}, n-2}\} \end{align*}`
en donde si `$t_{\hat{\beta}_{i}} \in RC$` para `$i=0,1$`, se rechaza
`$H_0$`. Mientras que el P-valor está dada por
`\begin{align*}  \text{P-valor}= 2\mathbb{P}(t_{n-2}\geq |t_{\hat{\beta}_i}|) \quad \text{ para } i = 0,1 \end{align*}`

en donde si P-valor es menor al nivel de significancia `$\alpha$`,
entonces se rechaza `$H_0$`.

**Nota**

-   Usualmente se quiere probar si `$\beta_{0}=0$` y `$\beta_{1}=0$`, ya
    que se quiere probar a significancia de los parámetros.
-   Observe que rechazar `$H_0:\beta_{1}=0$`, en la prueba de
    significancia, permite afirmar que la relación entre las variables
    `$Y$` y `$X$` se pueden aproximar mediante una linea recta.
-   Si no se rechaza `$H_0:\beta_{1}=0$`, significa que independiente
    del valor de `$X$`, el valor de `$Y$` será el mismo, y por tanto, no
    habrá una relación lineal entre `$Y$` y `$X$`.

### Intervalo de confianza

El intervalo de confianza par `$\beta_0$` está dado por
`\begin{align*}  \hat{\beta}_0 - t_{\frac{\alpha}{2}, n-2} \sqrt{\left(\frac{1}{n}+\frac{\bar{X}^2}{S_{xx}}\right)\sigma^2_{e}} < \beta_0 < \hat{\beta}_0 + t_{\frac{\alpha}{2}, n-2} \sqrt{\left(\frac{1}{n}+\frac{\bar{X}^2}{S_{xx}}\right)\sigma^2_{e}} \end{align*}`

para el caso de `$\beta_1$`, el intervalo de confianza está dado por
`\begin{align*}  \hat{\beta}_1 - t_{\frac{\alpha}{2}, n-2} \sqrt{\frac{\sigma^2_{e}}{S_{xx}}} < \beta_1 < \hat{\beta}_1 + t_{\frac{\alpha}{2}, n-2} \sqrt{\frac{\sigma^2_{e}}{S_{xx}}} \end{align*}`

**Nota**

Si el Intervalo de Confianza de `$\beta_1$` no contiene el cero, se
puede afirmar que la variable `$Y$` está relacionada linealmente con la
variable independiente `$X$`. En caso contrario, no existe relación
lineal entre `$X$` y `$Y$`, es decir, la variable `$X$` no es adecuada
para predecir el comportamiento de `$Y$`.

### Analisis de varianza

Una forma equivalente de hacer la prueba de significancia de la
regresión `$H_0:\beta_1=0$` vs `$H_1:\beta_1\neq0$` es a partir del
análisis de varianza, donde se descompone la varianza total de `$Y$`
(`$SCT$`) en dos fuentes, la variabilidad explicada por el modelo
(`$SCR$`) y la variabilidad no explicada por el modelo (`$SCE$`).

-   `$SCT = \sum_{i=1}^n (y_i-\bar{Y})^2$`
-   `$SCR = \sum_{i=1}^n (\hat{y}_i-\bar{Y})^2$`
-   `$SCE = \sum_{i=1}^n (y_i-\hat{y}_i)^2$`

Observe que las diferencias de las `$y_i$` con a su media pueden
escribirse como

`\begin{align*}  (y_i - \bar{Y}) = (\hat{y} - \bar{Y}) + (\underbrace{y_i - \hat{y_i}}_{e_i}) \end{align*}`

lo cual puede reescribirse como
`\begin{align*}  SCT = SCR + SCE \end{align*}`

**Nota**: Cada suma de cuadrados tiene asociado un número de grados de
libertad diferentes, tal que
`\begin{align*}  \stackrel{SCT}{n-1} = \stackrel{SCR}{1} + \stackrel{SCE}{n-2} \end{align*}`

con base en lo anterior es posible construir estimadores independientes
de `$\sigma^2$`, usando la respectiva suma de cuadrados dividido sus
grados de libertad, tal que

`\begin{align*}  MSR = \frac{SCR}{1} \quad \quad MSE = \frac{SCE}{n-2} \end{align*}`

finalmente se establece el estadítico de prueba bajo el supuesto de
normalidad
`\begin{align*}  F_c= \frac{SCR / 1}{SCE / n-2} \sim F_{1, n-2} \end{align*}`

En donde el criterio de decisión estará dado por el P-valor el cual
estará dado por
`\begin{align*}  \text{P-valor} = 2\times min(\mathbb{P}(F_{1,n-2} < F_c), \mathbb{P}(F_{1,n-2} > F_c)) \end{align*}`

donde si P-valor es menor al nivel de significancia `$\alpha$` se
rechaza la hipótesis nula y se concluye que el modelo lineal propuesto
es significativo para explicar el comportamiento `$Y$`.

### Coeficiente de determinación

Una forma de medir la bondad del ajuste del modelo de regresión es
mediante el coeficiente de determinación `$R^2$`, el cual se define como
`\begin{align*}  R^2=\frac{SCR}{SCT} = 1 - \frac{SCE}{SCT} \end{align*}`

y representa la proporción de variación total de `$Y$`, explicada por su
relación lineal con `$X$`.

Dado que `$R^2$` se encuentra entre `$0$` y `$1$`, entonces valores
cercanos a cero indican que la relación entre `$X$` y `$Y$` es muy
pobre, mientras que, valores cercanos a uno, indican que la recta
ajustada se aproxima relativamente bien a los puntos.

**Nota**: Un `$R^2$`alto no garantiza necesariamente que el modelo
regresión lineal ajustado sea adecuado para los datos, debido que hay
factores que afectan a este valor, como lo es el número de datos usados.

### Respuesta media

Una importante utilidad del modelo de regresión es que nos permite la
estimación de la media de la distribución de `$Y$` para un valor dado de
`$X$`, y además nos permite realizar predicciones sobre una nueva
variable `$y_0$` correspondiente a un nivel especificado de variables
`$x_0$`.

Considere un valor determinado `$x = x_0$`, y el objetivo será estimar
su respuesta media `$\mathbb{E}(Y|x_0)$`. La estimación puede ser
puntual o por intervalo, y donde, la estimación es válida solo para
valores de `$x_0$` dentro del rango de valores originales de `$X$`, que
se usaron para ajustar el modelo.

Entonces el estimador puntual de la respuesta media de `$Y$` dado
`$x_0$`
`\begin{align*}  \hat{y_0}= \hat{\mathbb{E}}(Y|x_0)= \hat{\beta}_0 + \hat{\beta}_1 x_0 \end{align*}`

donde se puede probar que
`\begin{align*}  \mathbb{E}(\hat{y_0}) &= \beta_0 + \beta_1 x_0 \\  Var(\hat{y_0}) &= \sigma^2_e\left[\frac{1}{n} + \frac{(x_0 - \bar{X})^2}{S_{xx}}\right]  \end{align*}`

Además, para un nivel de confianza del `$100(1-\alpha)\%$` un intervalo
de confianza para la respuesta media será de la forma

`\begin{align*}  y_0 \pm t_{\frac{\alpha}{2}, n-2} \sqrt{\sigma^2_e\left[\frac{1}{n}+ \frac{(x_0 - \bar{X})^2}{S_{xx}}\right]}  \end{align*}`
