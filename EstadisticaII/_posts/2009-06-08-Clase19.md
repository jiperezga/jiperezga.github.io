---
layout: post
title: "Clase 19"
main-class: 'clase'
permalink: /EstadisticaII/EstII:title.html
tags:

introduction: |
              Análisis de Regresión <br>
              - Análisis de varianza <br>
              - Coeficiente de determinación <br>
              - Respuesta medio <br>
              - Predicción de nuevas observaciones<br>

              
header-includes:
   - \usepackage{amsmath,amssymb,amsthm,amsfonts}
   - \usepackage[sectionbib]{natbib}
   - \usepackage[hidelinks]{hyperref}
output:
  md_document:
    variant: markdown_strict+backtick_code_blocks+autolink_bare_uris+ascii_identifiers+tex_math_single_backslash
    preserve_yaml: TRUE
always_allow_html: yes   
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding,
  output_dir = "../../EstadisticaII/_posts/", output_format = "all"  ) })
bibliography: "../../referencias.bib"
csl: "../../apa.csl"
link-citations: yes
---







## Análisis de Regresión

### Analisis de varianza

Una forma equivalente de hacer la prueba de significancia de la
regresión `$H_0:\beta_1=0$` vs `$H_1:\beta_1\neq0$` es a partir del
análisis de varianza, donde se descompone la varianza total de `$Y$`
(`$SCT$`) en dos fuentes, la variabilidad explicada por el modelo
(`$SCR$`) y la variabilidad no explicada por el modelo (`$SCE$`).

-   `$SCT = \sum_{i=1}^n (y_i-\bar{Y})^2$`
-   `$SCR = \sum_{i=1}^n (\hat{y}_i-\bar{Y})^2$`
-   `$SCE = \sum_{i=1}^n (y_i-\hat{y}_i)^2$`

Observe que las diferencias de las `$y_i$` con a su media `$\bar{Y}$`
pueden escribirse como

`\begin{align*}  (y_i - \bar{Y}) = (\hat{y}_i - \bar{Y}) + (\underbrace{y_i - \hat{y_i}}_{e_i}) \end{align*}`

lo cual puede reescribirse como
`\begin{align*}  SCT = SCR + SCE \end{align*}`

al aplicar la función de sumatoria a ambos lados de la función.

**Nota**

a)Para llegar a la ecuación anterior se puede verificar que
`\begin{align*}  \sum_{i=1}^{n}(\hat{y}_i - \bar{Y})e_i = \sum_{i=1}^{n}(\hat{y}_i - \bar{Y}) (y_i - \hat{y_i}) \end{align*}`

b)La expresiones de `$SCT$`, `$SCR$` y `$SCE$` pueden ser reescritas
como
`\begin{align*}  &SST = \sum_{i=1}^{n}y_i^2 - n\bar{Y}\\  &SSR = \hat{\beta}_1S_{xy} = \hat{\beta}_1^2 S_{xx}\\  &\text{y luego se podrá calcular }SCE \text{ como}\\  &SCE = SST - SSR \end{align*}`

c)Cada suma de cuadrados tiene asociado un número de grados de libertad
diferentes, tal que
`\begin{align*}  \underbrace{SCT}_{n-1} = \underbrace{SCR}_{1} + \underbrace{SCE}_{n-2} \end{align*}`

con base en lo anterior es posible construir estimadores independientes
de `$\sigma^2$`, usando la respectiva suma de cuadrados dividido sus
grados de libertad, tal que

`\begin{align*}  MSR = \frac{SCR}{1} \quad \quad MSE = \frac{SCE}{n-2} \end{align*}`

finalmente se establece el estadítico de prueba bajo el supuesto de
normalidad
`\begin{align*}  F_c= \frac{SCR / 1}{SCE / n-2} \sim F_{1, n-2} \end{align*}`

En donde el criterio de decisión estará dado, la región crítica, la cual
estará dada por
`\begin{align*}  RC:\{F|F>F_{\alpha, 1, n-2}\} \end{align*}`

o el P-valor, el cual estará dado por
`\begin{align*}  \text{P-valor} =  \mathbb{P}(F_{1,n-2} > F_c) \end{align*}`

donde si P-valor es menor al nivel de significancia `$\alpha$` se
rechaza la hipótesis nula y se concluye que el modelo lineal propuesto
es significativo para explicar el comportamiento `$Y$`.

### Coeficiente de determinación

Una forma de medir la bondad del ajuste del modelo de regresión es
mediante el coeficiente de determinación `$R^2$`, el cual se define como
`\begin{align*}  R^2=\frac{SCR}{SCT} = 1 - \frac{SCE}{SCT} \end{align*}`

y representa la proporción de variación total de `$Y$`, explicada por su
relación lineal con `$X$`.

Dado que `$R^2$` se encuentra entre `$0$` y `$1$`, entonces valores
cercanos a cero indican que la relación entre `$X$` y `$Y$` es muy
pobre, mientras que, valores cercanos a uno, indican que la recta
ajustada se aproxima relativamente bien a los puntos.

**Nota**: Un `$R^2$` alto no garantiza necesariamente que el modelo
regresión lineal ajustado sea adecuado para los datos, debido que hay
factores que afectan a este valor, como lo es el número de datos usados.

## Respuesta media y predicción de nuevas observaciones

Una importante utilidad del modelo de regresión es que nos permite la
estimación de la media de la distribución de `$Y$` para un valor dado de
`$X$`, y además nos permite realizar predicciones sobre una nueva
variable `$y_0$` correspondiente a un nivel especificado de variables
`$x_0$`.

### Respuesta media

Considere un valor determinado `$x = x_0$`, y el objetivo será estimar
su respuesta media `$\mathbb{E}(Y|x_0)$`. La estimación puede ser
puntual o por intervalo, y donde, **la estimación es válida solo para
valores de `$x_0$` dentro del rango de valores originales de `$X$`, que
se usaron para ajustar el modelo**.

Entonces el estimador puntual de la respuesta media de `$Y$` dado
`$x_0$`
`\begin{align*}  \hat{y_0}= \hat{\mathbb{E}}(Y|x_0)= \hat{\beta}_0 + \hat{\beta}_1 x_0 \end{align*}`

donde se puede probar que
`\begin{align*}  \mathbb{E}(\hat{y_0}) &= \beta_0 + \beta_1 x_0 \\  Var(\hat{y_0}) &= \hat{\sigma}^2_e\left[\frac{1}{n} + \frac{(x_0 - \bar{X})^2}{S_{xx}}\right]  \end{align*}`

Además, para un nivel de confianza del `$100(1-\alpha)\%$` un intervalo
de confianza para la respuesta media será de la forma

`\begin{align*}  \hat{y}_0 \pm t_{\frac{\alpha}{2}, n-2} \sqrt{\hat{\sigma}^2_e\left[\frac{1}{n}+ \frac{(x_0 - \bar{X})^2}{S_{xx}}\right]} \end{align*}`

Es de anotar que si se decide construir un intervalo de confianza para
todos los posibles valores de X, se obtendrán las bandas de confianza
del `$100(1-\alpha)\%$` para la respuesta media `$\beta_0+\beta_1x_i$`

### Predicción de nuevas observaciones

Suponga que `$x_0$` es un valor de interés que para predecir, entonces
para obtener un valor único `$y_0$`, podremos utilizar la fórmula de la
recta de regresión tal que, al igual que en el caso de la respuesta
media la estimación puntual media de `$Y$` dado `$x_0$`
`\begin{align*}  \hat{y_0}= \hat{\mathbb{E}}(Y|x_0)= \hat{\beta}_0 + \hat{\beta}_1 x_0 \end{align*}`

Pero en este al ser un valor predicho tendremos que estimar la varianza
de la diferencia `$\hat{y}_0 - y_0$`, la cual tiene una media
`\begin{align*}  \mathbb{E}(\hat{y_0} - y_0) &=\mathbb{E}[\hat{\beta}_0+\hat{\beta}_1x_0] - (\beta_0 + \beta_1 x_0 +\varepsilon)] = 0 \end{align*}`
y varianza \\
`\begin{align*}   Var(\hat{y_0} - y_0) &= \sigma^2\left[\frac{1}{n} + \frac{(x_0 - \bar{X})^2}{S_{xx}}\right] + \sigma^2\\   &=\sigma^2\left[1 + \frac{1}{n} + \frac{(x_0 - \bar{X})^2}{S_{xx}}\right]\\   &=\hat{\sigma}^2_e\left[1 + \frac{1}{n} + \frac{(x_0 - \bar{X})^2}{S_{xx}}\right]\\ \end{align*}`

Entonces, un intervalo de confianza del `$100(1-\alpha)\%$` para un
valor predicho `$y_0$`, se puede construir de la forma

`\begin{align*}  y_0 \pm t_{\frac{\alpha}{2}, n-2} \sqrt{\hat{\sigma}^2_e\left[1+\frac{1}{n}+ \frac{(x_0 - \bar{X})^2}{S_{xx}}\right]} \end{align*}`

Es de anotar que si se decide construir un intervalo de confianza para
todos los posibles valores de X, se obtendrán las bandas de confianza
del `$100(1-\alpha)\%$` para la predicción individual.
